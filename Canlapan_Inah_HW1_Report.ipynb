{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concept Questions\n",
    "\n",
    "#### 1. Whatâ€™s the main difference between supervised and unsupervised learning?  \n",
    "Supervised learning uses labeled datasets to train models/algorithms in order to classify data or predict outcomes. Unsupervised learning uses unlabeled data sets and allows machine learning algorithms to find patterns and information within the data. Results from supervised learning is shaped by the labelled input while results from unsupervised learning are less predictable. \n",
    "  \n",
    "  \n",
    "#### 2. Will different initializations for k-means lead to different results?  \n",
    "Yes. Minimizing the average square distances from each data point to its respective cluster is a convex optimization problem that may have several local minima. Choosing different initializations may lead to different local minima. \n",
    "  \n",
    "  \n",
    "#### 3. Give a short proof (can be in words but using correct logic) why k-means algorithm will converge in finite number of iterations.  \n",
    "K-means will always converge in a finite number of iterations because there is a finite number of ways a data set can be segmented into k different clusters. The number may be extremely large but it is still finite. \n",
    "  \n",
    "  \n",
    "#### 4. What is the main difference between k-means and generalized k-means algorithm? Explain how the choice of the similarity/dissimilarity/distance will impact the result.  \n",
    "The main difference between k-means and generalized k-means algorithm is in the way the generalized algorithm adjusts the cluster centers. Instead of adjusting the center to be cluster's mean like in k-means, the generalized approach instead solves an optimization problem to find the next centroid. \n",
    "  \n",
    "  \n",
    "#### 5. Consider the following simple graph. Write down the graph Laplacian matrix and find the eigenvectors associated with the zero eigen- value. Explain how do you find out the number of disconnected clusters in graph and identify these disconnected clusters using these eigenvectors.   \n",
    "   \n",
    "First, I calculate A, D and L. \n",
    "\n",
    "$A = \\begin{bmatrix}   0 & 1 & 1 & 0 & 0 \\\\ \n",
    "                        1 & 0 & 1 & 0 & 0 \\\\ \n",
    "                        1 & 1 & 0 & 0 & 0 \\\\\n",
    "                        0 & 0 & 0 & 0 & 1 \\\\\n",
    "                        0 & 0 & 0 & 1 & 0 \\\\ \\end{bmatrix}, \\;\\;\n",
    "D =  \\begin{bmatrix}    2 & 0 & 0 & 0 & 0 \\\\ \n",
    "                        0 & 2 & 0 & 0 & 0 \\\\ \n",
    "                        0 & 0 & 2 & 0 & 0 \\\\\n",
    "                        0 & 0 & 0 & 1 & 0 \\\\\n",
    "                        0 & 0 & 0 & 0 & 1 \\\\ \\end{bmatrix}, \\;\\; \n",
    "L = D-A = \\begin{bmatrix}    2 & -1 & -1 & 0 & 0 \\\\ \n",
    "                             -1 & 2 & -1 & 0 & 0 \\\\ \n",
    "                             -1 & -1 & 2 & 0 & 0 \\\\\n",
    "                             0 & 0 & 0 & 1 & -1 \\\\\n",
    "                             0 & 0 & 0 & -1 & 1 \\\\ \\end{bmatrix} \\;\\; $  \n",
    "                               \n",
    "To find out the number of disconnected clusters in the graph, I used the python code in the cell below to perform eigenvalue decomposition. Then I found the columns in the eigenvector matrix that correspond to a 0 eigenvalue. To identify the disconnected clusters, I take the indexes of the 0 eigenvalues and find the corresponding column in the eigenvector matrix. Based on the resulting matrix, I can conclude that nodes 1, 2 and 3 are connected in one cluster while nodes 4 and 5 are connected in another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [ 3. -0.  3.  2.  0.]\n",
      "Indexes of 0 eigenvalues: [1, 4]\n",
      "Eigenvectors: \n",
      "[[ 0.81649658 -0.57735027  0.30959441  0.          0.        ]\n",
      " [-0.40824829 -0.57735027 -0.80910101  0.          0.        ]\n",
      " [-0.40824829 -0.57735027  0.49950661  0.          0.        ]\n",
      " [ 0.          0.          0.          0.70710678  0.70710678]\n",
      " [ 0.          0.          0.         -0.70710678  0.70710678]]\n",
      "Cluster Assignment:\n",
      "[[-0.57735027  0.        ]\n",
      " [-0.57735027  0.        ]\n",
      " [-0.57735027  0.        ]\n",
      " [ 0.          0.70710678]\n",
      " [ 0.          0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set up matrices\n",
    "A = np.array([[0,1,1,0,0], [1,0,1,0,0], [1,1,0,0,0], [0,0,0,0,1], [0,0,0,1,0]])\n",
    "D = np.array([[2,0,0,0,0], [0,2,0,0,0], [0,0,2,0,0], [0,0,0,1,0], [0,0,0,0,1]])\n",
    "L = D-A\n",
    "\n",
    "# Eigenvalue decomposition\n",
    "s, v = np.linalg.eig(L)\n",
    "\n",
    "# Get index of eigenvalues that are 0\n",
    "zero_evalue_index =  [i for i, x in enumerate(np.around(s,1)) if x == 0]\n",
    "print(\"Eigenvalues: \" + str(np.around(s,1)))\n",
    "print(\"Indexes of 0 eigenvalues: \" + str(zero_evalue_index))\n",
    "\n",
    "# Columns corresponding to the eigenvalues that are 0\n",
    "print(\"Eigenvectors: \")\n",
    "print(str(v))\n",
    "print(\"Cluster Assignment:\")\n",
    "print(str(v[:, zero_evalue_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Compression Using Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Georgia Tech\n",
    "  \n",
    "| k  | (L2) Iterations  | (L2) Run Time (Seconds)    | (L1) Iterations  | (L1) Run Time (Seconds)    |\n",
    "|:--:|:----------------:|:--------------------------:|:----------------:|:--------------------------:|\n",
    "| 2  | 5                | 72                         | 5                | 59                         | \n",
    "| 4  | 13          | 230                   | 13          | 298                   | \n",
    "| 8  | 35          | 1018                  | 68          | 2432                  |  \n",
    "| 16 | 33          | 2054                  | 50          | 3656                 |\n",
    "\n",
    "   \n",
    "  \n",
    "  \n",
    "  \n",
    "Image Comparison:\n",
    "\n",
    "| k  | Original    | L2     | L1   | \n",
    "|:--:|:-----------:|:-------:|:------: |\n",
    "| 2  | <img src=\"data/GeorgiaTech.bmp\" width=\"300\" />| <img src=\"Q2_output/GeorgiaTech_2.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/GeorgiaTech_2.png\" width=\"300\" />|\n",
    "| 4  | <img src=\"data/GeorgiaTech.bmp\" width=\"300\" />| <img src=\"Q2_output/GeorgiaTech_4.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/GeorgiaTech_4.png\" width=\"300\" />|\n",
    "| 8  | <img src=\"data/GeorgiaTech.bmp\" width=\"300\" />| <img src=\"Q2_output/GeorgiaTech_8.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/GeorgiaTech_8.png\" width=\"300\" />|\n",
    "| 16 | <img src=\"data/GeorgiaTech.bmp\" width=\"300\" />| <img src=\"Q2_output/GeorgiaTech_16.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/GeorgiaTech_16.png\" width=\"300\" />|\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Football\n",
    "\n",
    "| k  | (L2) Iterations  | (L2) Run Time (Seconds)    | (L1) Iterations  | (L1) Run Time (Seconds)    |\n",
    "|:--:|:----------------:|:--------------------------:|:----------------:|:--------------------------:|\n",
    "| 2  | 10               | 169                        | 10               | 204                         | \n",
    "| 4  | 13          | 431                   | 10           | 351                   | \n",
    "| 8  | 20          | 1125                   | 19          |  1354                  |  \n",
    "| 16 | 33          | 3114                  |  33         |  4259                 |\n",
    "\n",
    "\n",
    "Image Comparison:\n",
    "\n",
    "| k  | Original    | L2     | L1   | \n",
    "|:--:|:-----------:|:-------:|:------: |\n",
    "| 2  | <img src=\"data/football.bmp\" width=\"300\" />| <img src=\"Q2_output/football_2.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/football_2.png\" width=\"300\" />|\n",
    "| 4  | <img src=\"data/football.bmp\" width=\"300\" />| <img src=\"Q2_output/football_4.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/football_4.png\" width=\"300\" />|\n",
    "| 8  | <img src=\"data/football.bmp\" width=\"300\" />| <img src=\"Q2_output/football_8.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/football_8.png\" width=\"300\" />|\n",
    "| 16 | <img src=\"data/football.bmp\" width=\"300\" />| <img src=\"Q2_output/football_16.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/football_16.png\" width=\"300\" /> |\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Beach\n",
    "\n",
    "| k  | (L2) Iterations  | (L2) Run Time (Seconds)    | (L1) Iterations  | (L1) Run Time (Seconds)    |\n",
    "|:--:|:----------------:|:--------------------------:|:----------------:|:--------------------------:|\n",
    "| 2  | 5                | 38                         |  6               |  56                        | \n",
    "| 4  | 14          | 168                   | 8          |   147                 | \n",
    "| 8  | 11          | 205                   | 10          |  259                  |  \n",
    "| 16 | 30          | 905                  |  23         |   1021                |\n",
    "\n",
    "\n",
    "Image Comparison:\n",
    "\n",
    "| k  | Original    | L2     | L1   | \n",
    "|:--:|:-----------:|:-------:|:------: |\n",
    "| 2  | <img src=\"data/beach.jpg\" width=\"300\" />| <img src=\"Q2_output/beach_2.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/beach_2.png\" width=\"300\" />|\n",
    "| 4  | <img src=\"data/beach.jpg\" width=\"300\" />| <img src=\"Q2_output/beach_4.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/beach_4.png\" width=\"300\" />|\n",
    "| 8  | <img src=\"data/beach.jpg\" width=\"300\" />| <img src=\"Q2_output/beach_8.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/beach_8.png\" width=\"300\" />|\n",
    "| 16 | <img src=\"data/beach.jpg\" width=\"300\" />| <img src=\"Q2_output/beach_16.png\" width=\"300\" /> | <img src=\"Q2_output/Part_2/beach_16.png\" width=\"300\" />|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing L1 and L2 Norm for Compression\n",
    "There were several differences I found when using the Euclidean vs Manhattan distance for image compression:\n",
    "\n",
    "1. Number of Iterations & Run Time  \n",
    "Across all values of k, there were several instances where the run time using L1 norm was longer even though compression was completed in the same or fewer number of iterations. In general, using the L2 norm to compress images was faster and completed in fewer iterations. \n",
    "\n",
    "Examples of where the L1 norm had a longer run time in the same or fewer iterations compared to L2:   \n",
    "\n",
    "| Image | k  | (L2) Iterations  | (L2) Run Time (Seconds)    | (L1) Iterations  | (L1) Run Time (Seconds)    |\n",
    "|:--:|:--:|:----------------:|:--------------------------:|:----------------:|:--------------------------:|\n",
    "|Georgia Tech |  4  | 13          | 230                   | 13          | 298                   | \n",
    "|football | 2  | 10               | 169                        | 10               | 204                         | \n",
    "|football | 8  | 20          | 1125                   | 19          |  1354                  |  \n",
    "|football | 16 | 33          | 3114                  |  33         |  4259                 |\n",
    "|beach | 8  | 11          | 205                   | 10          |  259                  |  \n",
    "|beach | 16 | 30          | 905                  |  23         |   1021                |\n",
    "\n",
    "2. Image Quality & Colours  \n",
    "In lower values of k (2,4,8), the colours chosen by the L1 norm are more cool toned, less vibrant and have less contrast than the colours chosen by L2. With the highest value of k, the images look more comparable. However, looking very closely at the Georgia Tech and Beach images, it appears that the L1 norm has more trouble distinguishing shades of blue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Political Blogs Dataset\n",
    "\n",
    "### Q1 Code Output\n",
    "\n",
    "k = 2  \n",
    "Majority: {0: 1, 1: 0}  \n",
    "Mismatch Rates: {0: 0.449, 1: 0.4636}  \n",
    "Average of Mismatch Rates = 0.45630000000000004  \n",
    "Spread of Mismatch Rates = (0.449, 0.4636) = 0.014600000000000002  \n",
    "  \n",
    "k = 5  \n",
    "Majority: {0: 1, 1: 1, 2: 0, 3: 0, 4: 1}  \n",
    "Mismatch Rates: {0: 0.0656, 1: 0.1311, 2: 0.0904, 3: 0.075, 4: 0.0175}  \n",
    "Average of Mismatch Rates = 0.07592  \n",
    "Spread of Mismatch Rates = (0.0175, 0.1311) = 0.11359999999999999  \n",
    "\n",
    "k = 10  \n",
    "Majority: {0: 1, 1: 0, 2: 1, 3: 1, 4: 1, 5: 0, 6: 1, 7: 1, 8: 0, 9: 0}  \n",
    "Mismatch Rates: {0: 0.0275, 1: 0.033, 2: 0.1711, 3: 0.3111, 4: 0.3889, 5: 0.0702, 6: 0.1235, 7: 0.0806, 8: 0.0827, 9: 0.0659}  \n",
    "Average of Mismatch Rates = 0.13545000000000001  \n",
    "Spread of Mismatch Rates = (0.0275, 0.3889) = 0.3614  \n",
    "\n",
    "k = 20  \n",
    "Majority: {0: 0, 1: 1, 2: 0, 3: 1, 4: 0, 5: 1, 6: 0, 7: 0, 8: 0, 9: 1, 10: 1, 11: 0, 12: 1, 13: 0, 14: 1, 15: 0, 16: 1, 17: 0, 18: 1, 19: 0}  \n",
    "Mismatch Rates: {0: 0.2083, 1: 0.0943, 2: 0.0536, 3: 0.027, 4: 0.0588, 5: 0.1299, 6: 0.0952, 7: 0.4286, 8: 0.2909, 9: 0.025, 10: 0.2, 11: 0.1091, 12: 0.0, 13: 0.0982, 14: 0.0682, 15: 0.1333, 16: 0.0526, 17: 0.0317, 18: 0.2308, 19: 0.0143}  \n",
    "Average of Mismatch Rates = 0.11749  \n",
    "Spread of Mismatch Rates = (0.0, 0.4286) = 0.4286  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "#### Tune your k and find the number of clusters to achieve a reasonably small mismatch rate. Please explain how you tune k and what is the achieved mismatch rate. Please explain intuitively what this results tells about the network community structure.\n",
    "\n",
    "As the number of clusters gets larger, the spread (max-min) of the cluster mismatch rates gets wider and the average of the mismatch rates also increases. This tells me that breaking up the blog site list into 10 to 20 communities is too many. \n",
    "\n",
    "In order to tune k, I will test breaking up the list of blog sites into 2 to 9 communities and compare metrics.\n",
    "Based on the results below, the size of community that has the lowest average mismatch rate and a small spread mismatch rate is 5. I didn't choose k = 4 (it had the smallest spread) because the minimum mismatch rate is more than 2x the minimum mismatch rate of k = 5. \n",
    "\n",
    "These results tell me that in the remaining 1224 political blogs (266 were removed because they weren't connected to any other sites), there are 5 communities where the connected blog sites share the same political orientation. At most, 13% of sites that belong to a community may not share the same political orientation. \n",
    "\n",
    "The achieved mismatch rates are as follows: \n",
    "* Cluster 0 =  0.1311\n",
    "* Cluster 1 = 0.0171\n",
    "* Cluster 2 = 0.0775\n",
    "* Cluster 3 = 0.0856\n",
    "* Cluster 4 = 0.1159\n",
    "\n",
    "#### Code Output\n",
    "k = 2  \n",
    "Majority: {0: 1, 1: 0}  \n",
    "Mismatch Rates: {0: 0.449, 1: 0.4636}  \n",
    "Average of Mismatch Rates = 0.45630000000000004  \n",
    "Spread of Mismatch Rates = (0.449, 0.4636) = 0.014600000000000002  \n",
    "\n",
    "k = 3  \n",
    "Majority: {0: 0, 1: 0, 2: 1}  \n",
    "Mismatch Rates: {0: 0.45, 1: 0.0912, 2: 0.0624}  \n",
    "Average of Mismatch Rates = 0.2012  \n",
    "Spread of Mismatch Rates = (0.0624, 0.45) = 0.3876  \n",
    "\n",
    "k = 4  \n",
    "Majority: {0: 1, 1: 1, 2: 0, 3: 0}  \n",
    "Mismatch Rates: {0: 0.135, 1: 0.0382, 2: 0.0921, 3: 0.1121}  \n",
    "Average of Mismatch Rates = 0.09435  \n",
    "Spread of Mismatch Rates = (0.0382, 0.135) = 0.09680000000000001  \n",
    "\n",
    "k = 5  \n",
    "Majority: {0: 1, 1: 1, 2: 0, 3: 0, 4: 1}  \n",
    "Mismatch Rates: {0: 0.1311, 1: 0.0171, 2: 0.0775, 3: 0.0856, 4: 0.1159}  \n",
    "Average of Mismatch Rates = 0.08544  \n",
    "Spread of Mismatch Rates = (0.0171, 0.1311) = 0.11399999999999999  \n",
    "\n",
    "k = 6  \n",
    "Majority: {0: 1, 1: 0, 2: 1, 3: 0, 4: 1, 5: 1}  \n",
    "Mismatch Rates: {0: 0.1169, 1: 0.0973, 2: 0.0221, 3: 0.0535, 4: 0.1408, 5: 0.1489}  \n",
    "Average of Mismatch Rates = 0.09658333333333334  \n",
    "Spread of Mismatch Rates = (0.0221, 0.1489) = 0.1268  \n",
    "\n",
    "k = 7  \n",
    "Majority: {0: 1, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 1}  \n",
    "Mismatch Rates: {0: 0.1261, 1: 0.0232, 2: 0.0536, 3: 0.0214, 4: 0.145, 5: 0.3143, 6: 0.1357}  \n",
    "Average of Mismatch Rates = 0.11704285714285714  \n",
    "Spread of Mismatch Rates = (0.0214, 0.3143) = 0.29290000000000005  \n",
    "\n",
    "k = 8  \n",
    "Majority: {0: 1, 1: 1, 2: 0, 3: 1, 4: 1, 5: 0, 6: 1, 7: 0}  \n",
    "Mismatch Rates: {0: 0.1858, 1: 0.0697, 2: 0.0376, 3: 0.2941, 4: 0.0219, 5: 0.1382, 6: 0.1185, 7: 0.1685}  \n",
    "Average of Mismatch Rates = 0.1292875  \n",
    "Spread of Mismatch Rates = (0.0219, 0.2941) = 0.2722  \n",
    "\n",
    "k = 9  \n",
    "Majority: {0: 1, 1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0, 7: 0, 8: 1}  \n",
    "Mismatch Rates: {0: 0.2551, 1: 0.1495, 2: 0.0855, 3: 0.0248, 4: 0.0741, 5: 0.0403, 6: 0.1064, 7: 0.0887, 8: 0.0595}  \n",
    "Average of Mismatch Rates = 0.09821111111111111  \n",
    "Spread of Mismatch Rates = (0.0248, 0.2551) = 0.2303  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
