{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c38567e",
   "metadata": {},
   "source": [
    "# HW 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05637b7",
   "metadata": {},
   "source": [
    "## 1. Optimization\n",
    "\n",
    "**1. Show step-by-step mathematical derivation for the gradient of the cost function l(θ) in (1).**  \n",
    "*from slide 40 from module 9*\n",
    "\n",
    "From logistic regression, we can derive the following probabilities:  \n",
    "$P(y=1|x, \\theta) = \\frac{1}{1+e^{-\\theta x}}$  \n",
    "$P(y=0|x, \\theta) = \\frac{e^{-\\theta x}}{1+e^{-\\theta x}}$\n",
    "\n",
    "Using the definition of the cost function, apply the log product rule:  \n",
    "$l(\\theta) := log \\Pi_{i=1}^{m} P(y^i|x^i, \\theta)$  \n",
    "$= log(\\Sigma_{i=1}^{m} P(y^i|x^i, \\theta))$  \n",
    "$= \\Sigma_{i=1}^{m} log(P(y^i|x^i, \\theta))$  \n",
    "$= \\Sigma_{i=1}^{m} y_ilog(P(y=1|x^i, \\theta)) + (1-y_i)log(P(y=0|x^i\\theta))$  \n",
    "\n",
    "Plugging in the definitions from above and simplifying:  \n",
    "$= \\Sigma_{i=1}^{m} y_ilog(\\frac{1}{1+e^{-\\theta x^i}}) + (1-y_i)log(\\frac{e^{-\\theta x^i}}{1+e^{-\\theta x^i}})$  \n",
    "$= \\Sigma_{i=1}^{m} y_i [log(1) - log(1+e^{-\\theta x^i})] + (1-y_i) [log(e^{-\\theta x^i}) - log(1+e^{-\\theta x^i})]$  \n",
    "$= \\Sigma_{i=1}^{m} y_i[- log(1+e^{-\\theta x^i}) + (1-y_i)[-\\theta x^i - log(1+e^{-\\theta x^i})] $  \n",
    "$= \\Sigma_{i=1}^{m} {-log(1+e^{-\\theta^Tx^i}) + (y^i-1)\\theta^Tx^i}$\n",
    "\n",
    "**2. Write a pseudo-code for performing gradient descent to find the optimizer $\\theta^*$. This is essentially what the training procedure does.**  \n",
    "\n",
    "Initialize $\\theta^0$ and number of max iterations.\n",
    "t = 1  \n",
    "\n",
    "while $||\\theta^{t+1} - \\theta^t|| \\gt \\epsilon $ and t < max_iterations:  \n",
    "$\\theta^{t+1} = \\theta^t + \\gamma_t\\Sigma_i(y^i-1)x^i + \\frac{exp({\\theta^t}^Tx^i)x^i}{1+ exp({\\theta^t}^Tx^i)x^i} $  \n",
    "t+=1\n",
    "\n",
    "**3. Write the pseudo-code for performing the stochastic gradient descent algorithm to solve the training of logistic regression problem (1). Please explain the difference between gradient descent and stochastic gradient descent for training logistic regression.**\n",
    "\n",
    "The stochastic gradient descent uses a small subset of data at each iteration, unlike in gradient descent where the full data set is used, to compute the gradient. \n",
    "\n",
    "Initialize $\\theta^0$ and number of max iterations.\n",
    "t = 1  \n",
    "i = 0\n",
    "\n",
    "while $||\\theta^{t+1} - \\theta^t|| \\gt \\epsilon $ and t < max_iterations:  \n",
    "\n",
    "select subset of data, $S_i$ and compute $\\theta^{t+1} using:$  \n",
    "$\\theta^{t+1} = \\theta^t + \\gamma_t\\Sigma_i(y^i-1)x^i + \\frac{exp({\\theta^t}^Tx^i)x^i}{1+ exp({\\theta^t}^Tx^i)x^i} $\n",
    "\n",
    "i+=1  \n",
    "t+=1\n",
    "\n",
    "\n",
    "**4. We will show that the training problem in basic logistic regression problem is concave. Derive the Hessian matrix of l(θ) and based on this, show the training problem (1) is concave. Explain why the problem can be solved efficiently and gradient descent will achieve a unique global optimizer, as we discussed in class.**\n",
    "\n",
    "$l(\\theta)$ is concave if $\\nabla^2 < 0$ (the Hessian is negative semidefinite).  \n",
    "$l(\\theta) = \\Sigma_{i=1}^{m} {-log(1+e^{-\\theta^Tx^i}) + (y^i-1)\\theta^Tx^i}$  \n",
    "$\\frac{\\partial l(\\theta)}{\\partial \\theta} = \\Sigma_{i=1}^{m} \\frac{x^i}{1+e^{x^i\\theta^T}} + x^i(y^i-1)$  \n",
    "$\\frac{\\partial^2 l(\\theta)}{\\partial \\theta^2} = \\Sigma_{i=1}^{m} -\\frac{{x^i}^2 e^{\\theta^T x^i}}{(e^{\\theta^T x^i} + x^i)^2} \\lt 0$, thus $l(\\theta)$ is concave.\n",
    "\n",
    "Since this is a concave function, there only exists a single global optimum and no local maximums. By using gradient descent, we can take small steps in the direction of the negative gradient. When the gradient is 0 or is very small, we know that we have arrived at the unique global optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b7969",
   "metadata": {},
   "source": [
    "## 2. Comparing Classifiers\n",
    "\n",
    "### 2.1 Divorce Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a69278",
   "metadata": {},
   "source": [
    "### Part A\n",
    "**Report testing accuracy for each of the three classifiers. Comment on their performance: which performs the best and make a guess why they perform the best in this setting.**  \n",
    "\n",
    "1. Logistic Regression\n",
    " * Train Accuracy = 1\n",
    " * Test Accuracy = 0.9412\n",
    " \n",
    "2. KNN, k=5\n",
    " * Train Accuracy = 0.9779\n",
    " * Test Accuracy = 0.9706\n",
    "\n",
    "3. Naive Bayes\n",
    " * Train Accuracy = 0.9779\n",
    " * Test Accuracy = 0.9706\n",
    "\n",
    "KNN and Naive Bayes peform best on the test data. KNN probably performs well because you'd expect that divorced couples would share some of the same characteristics and thus, would be close 'neighbours' to each other. I didn't expect Naive Bayes to work well because it assumes all features are independent given the label. Looking at the description of the features, I would say the some of them are dependent on another. For example:  \n",
    "\n",
    "5. The time I spent with my wife is special for us.\n",
    "6. We don't have time at home as partners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35033da",
   "metadata": {},
   "source": [
    "### Part B\n",
    "**Now perform PCA to project the data into two-dimensional space. Build the classifiers (Naive Bayes, Logistic Regression, and KNN) using the two-dimensional PCA results. Plot the data points and decision boundary of each classifier in the two-dimensional space. Comment on the difference between the decision boundary for the three classifiers. Please clearly represent the data points with different labels using different colors.**\n",
    "\n",
    "<img src=\"Q2.1_PCA_Output.png\" width=\"650\"/>\n",
    "\n",
    "1. Logistic Regression\n",
    " * Train Accuracy = 0.9779\n",
    " * Test Accuracy = 0.9706\n",
    " \n",
    "2. KNN, k=5\n",
    " * Train Accuracy = 0.9853\n",
    " * Test Accuracy = 0.9706\n",
    "\n",
    "3. Naive Bayes\n",
    " * Train Accuracy = 0.9779\n",
    " * Test Accuracy = 0.9706\n",
    " \n",
    "Applying PCA, we see an improvement in the test accuracy for Logistic Regression, making it perform equally as well as KNN and Naive Bayes. The decision boundaries for Logistic Regression and Naive Bayes are linear while the KNN decision boundary is irregular and noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9b80f",
   "metadata": {},
   "source": [
    "### 2.2 Handwritten Digits Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c0ec1",
   "metadata": {},
   "source": [
    "### Part A\n",
    "**Report confusion matrix, precision, recall, and F-1 score for each of the classifiers.**\n",
    "\n",
    "<img src=\"Q2.2_KNN.png\" width=\"400\"/>\n",
    "<img src=\"Q2.2_LogisticRegression.png\" width=\"400\"/>\n",
    "<img src=\"Q2.2_SVM.png\" width=\"400\"/>\n",
    "<img src=\"Q2.2_KernelSVM.png\" width=\"400\"/>\n",
    "<img src=\"Q2.2_NeuralNetworks.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7fad2",
   "metadata": {},
   "source": [
    "### Part B \n",
    "**Comment on the performance of the classifier and give your explanation why some of them perform better than the others.**  \n",
    "\n",
    "<u>Test Accuracy Summary</u>\n",
    "1. Neural Networks = 0.9572\n",
    "2. Kernel SVM = 0.9545\n",
    "3. KNN = 0.9366\n",
    "4. Logistic Regression = 0.9256\n",
    "5. SVM = 0.9121\n",
    "\n",
    "The worst performing classifier is SVM; this is likely because the data can't be separated well using linear boundaries. The best performing classifier is the neural network and not far behind is the Kernel SVM. This is likely the case because of the same reason why linear SVM is the worst performing classifier - the data is better classified using non-linear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449b65a",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes for Spam Filtering\n",
    "\n",
    "### Part 1\n",
    "**Calculate class prior P(y = 0) and P(y = 1) from the training data, where y = 0 corresponds to spam messages, and y = 1 corresponds to non-spam messages. Note that these class prior essentially corresponds to the frequency of each class in the training sample. Write down the feature vectors for each spam and non-spam messages.**  \n",
    "Let 1 = spam, 0 = not spam.\n",
    "\n",
    "V = {secret, offer, low, price, valued, customer, today, dollar, million, sports, is, for, play, healthy, pizza}.\n",
    "\n",
    "|i|Phrase| Feature Vector | y|\n",
    "|:-|:-----| :----: | :----: | \n",
    "|1|million dollar offer|{0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0} |1|\n",
    "|2|secret offer today|{1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0}|1|\n",
    "|3|secret is secret|{2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0}|1|\n",
    "|4|low price for valued customer|{0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0}|0|\n",
    "|5|play secret sports today|{1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0}|0|\n",
    "|6|sports is healthy|{0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0}|0|\n",
    "|7|low price pizza|{0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1}|0|\n",
    "\n",
    "$P(y = 0) = \\frac{4}{7}$  \n",
    "$P(y = 1) = \\frac{3}{7}$\n",
    "\n",
    "### Part 2\n",
    "**Calculate the maximum likelihood estimates of $\\theta_{0,1},\\theta_{0,7},\\theta_{1,8},\\theta_{1,15}$ by maximizing the log-likelihood function above.**\n",
    "\n",
    "$l(\\theta_{0,1},...,\\theta_{0,d},\\theta_{1,1},...,\\theta_{1,d}) = \\Sigma_{i=1}^{7}\\Sigma_{k=1}^{15} x_k^ilog(\\theta_{y^i,k})$  \n",
    "\n",
    "$max \\;\\; l(\\theta_{0,1},...,\\theta_{0,d},\\theta_{1,1},...,\\theta_{1,d}) = \\Sigma_{i=1}^{7}\\Sigma_{k=1}^{15} x_k^ilog(\\theta_{y^i,k})$  \n",
    "such that $\\Sigma_{k=1}^{15}\\theta_{y^i,k} = 1, y^i={0,1} $  \n",
    "\n",
    "Add the Lagrangian multiplier:  \n",
    "$max \\;\\; l(\\theta_{0,1},...,\\theta_{0,d},\\theta_{1,1},...,\\theta_{1,d}) = \\Sigma_{i=1}^{7}\\Sigma_{k=1}^{15} x_k^ilog(\\theta_{y^i,k}) + \\lambda(\\Sigma_{k=1}^{15}\\theta_{y^i,k}-1)$  \n",
    "\n",
    "Now take derivative wrt to specific $\\theta$ and set to 0. I will use $\\theta_{0,1}$ for this example:\n",
    "\n",
    "$\\frac{\\partial l{\\theta...}}{\\partial \\theta_{0,1}} = \\Sigma_{i=4,5,6,7} \\frac{x_1^i}{\\theta_{0,1}}+\\lambda = 0 \\longrightarrow$ need to sum over all $x_k^i$ where $y^i=0$ or i=4,5,6,7 and k=1.  \n",
    "$-\\lambda = \\frac{1}{\\theta_{0,1}} \\Sigma_{i=4,5,6,7}x_1^i$  \n",
    "$\\theta_{0,1} = - \\frac{\\Sigma_{i=4,5,6,7}x_1^i}{\\lambda} = -\\frac{1}{\\lambda}$  \n",
    "\n",
    "I got stuck here solving for $\\lambda$. It has to be negative in order to cancel out the negative in the numerator. However, I know that the value for $\\theta_{0,1}$ is supposed to be the probability of the word k appearing in class c. So if the numerator is the number times of word 1 (secret) appears in sentences with class = 0, I will assume that the denominator is the total number of words that appear in class = 0 in order to finish the question.\n",
    "\n",
    "Thus, $\\theta_{0,1} = \\frac{1}{14}$\n",
    "\n",
    "Following this same logic for the rest of the $\\theta$:  \n",
    "$\\theta_{0,7} = - \\frac{\\Sigma_{i=4,5,6,7}x_7^i}{\\lambda} = -\\frac{1}{\\lambda} = \\frac{1}{14}$  \n",
    "$\\theta_{1,8} = - \\frac{\\Sigma_{i=1,2,3}x_8^i}{\\lambda} = -\\frac{1}{\\lambda} = \\frac{1}{9}$  \n",
    "$\\theta_{1,15} = - \\frac{\\Sigma_{i=1,2,3}x_15^i}{\\lambda} = 0$\n",
    "\n",
    "\n",
    "### Part 3\n",
    "**Given a test message \"today is secret\", using the Naive Bayes classifier that you have trained in Part (a)-(b), calculate the posterior and decide whether it is spam or not spam.**\n",
    "\n",
    "$q_1$ = P(y=1|'today is secret') = P('today is secret'|y=1)P(y=1)/(P(today is secret'|y=0)P(y=0) + P(today is secret'|y=1)P(y=1))\n",
    "\n",
    "$q_0$ = P(y=0|'today is secret') = P('today is secret'|y=0)P(y=0)/(P(today is secret'|y=0)P(y=0) + P(today is secret'|y=1)P(y=1))\n",
    "\n",
    "From part 1:  \n",
    "$P(y = 0) = \\frac{4}{7}$  \n",
    "$P(y = 1) = \\frac{3}{7}$\n",
    "\n",
    "To calculate P('today is secret'|y=1), need to sum $\\theta_{1,7}, \\theta_{1,11}, \\theta_{1,1}$:  \n",
    "$\\theta_{1,7} = $ (# times of word 7 (today) appears in sentences with class = 1)/(# words in sentences with class = 1) $= \\frac{1}{9}$  \n",
    "$\\theta_{1,11} = $ (# times of word 11 (is) appears in sentences with class = 1)/(# words in sentences with class = 1) $= \\frac{1}{9}$  \n",
    "$\\theta_{1,1} = $ (# times of word 1 (secret) appears in sentences with class = 1)/(# words in sentences with class = 1) $= \\frac{3}{9}$  \n",
    "$\\longrightarrow$ P('today is secret'|y=1) = $\\frac{4}{9}$\n",
    "\n",
    "To calculate P('today is secret'|y=0), need to sum $\\theta_{0,7}, \\theta_{0,11}, \\theta_{0,1}$  \n",
    "$\\theta_{0,7} = $ (# times of word 7 (today) appears in sentences with class = 0)/(# words in sentences with class = 0) $= \\frac{1}{14}$  \n",
    "$\\theta_{0,11} = $ (# times of word 11 (is) appears in sentences with class = 0)/(# words in sentences with class = 0) $= \\frac{1}{14}$  \n",
    "$\\theta_{0,1} = $ (# times of word 1 (secret) appears in sentences with class = 0)/(# words in sentences with class = 0) $= \\frac{1}{14}$\n",
    "\n",
    "$\\longrightarrow$ P('today is secret'|y=0) = $\\frac{3}{14}$  \n",
    "\n",
    "$q_1$ = P(y=1|'today is secret') = $\\frac{\\frac{4}{9}\\frac{3}{7}}{\\frac{4}{9}\\frac{3}{7} + \\frac{3}{14}\\frac{4}{7}} = \\frac{14}{23}$\n",
    "\n",
    "\n",
    "$q_0$ = P(y=0|'today is secret') = $\\frac{\\frac{3}{14}\\frac{4}{7}}{\\frac{4}{9}\\frac{3}{7} + \\frac{3}{14}\\frac{4}{7}} = \\frac{9}{23}$\n",
    "\n",
    "$\\longrightarrow$ Since P(y=1|'today is secret') > P(y=0|'today is secret'), \"today is secret\" is **spam**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
