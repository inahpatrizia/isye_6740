{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449dcc42",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2736f",
   "metadata": {},
   "source": [
    "## 1. Conceptual Questions\n",
    "**1.  Based on the outline given in the lecture, show mathemtically that the maximum likelihood estimate (MLE) for Gaussian mean and variance parameters are given by**   \n",
    "$\\hat\\mu = \\frac{1}{m} \\Sigma^{m}_{i=1}x^i, \\;\\; \\hat\\sigma^2 = \\frac{1}{m} \\Sigma^{m}_{i=1} (x^i - \\hat\\mu)^2$\n",
    "\n",
    "   **Note: For this derivation, you will also need to show that these estimates for μ and σ\n",
    "are maximum.**\n",
    "\n",
    "First, start with the Gausian distribution:   \n",
    "$p(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-1}{2\\sigma^2}(x-\\mu)^2}$\n",
    "\n",
    "Then, take the log to get the log likelihood:  \n",
    "$l(\\mu, \\sigma, D) = \\frac{-m}{2}log(2\\pi) - \\frac{m}{2}log(\\sigma^2) - \\Sigma_{i=1}^{m}\\frac{(x^i-\\mu)^2}{2\\sigma^2}$\n",
    "\n",
    "Maximize $l(\\mu, \\sigma, D)$ with respect to $\\mu$ and $\\sigma^2$ and set to 0 to get the estimates:  \n",
    "$\\frac{\\partial l}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu}\\frac{-1}{2\\sigma^2} \\Sigma_{i=1}^{m}(x^i-\\mu)^2\n",
    "= \\frac{-1}{2\\sigma^2} \\Sigma_{i=1}^{m} 2(x^i-\\mu)(-1)  $  \n",
    "$0 = \\frac{1}{\\sigma^2} (\\Sigma_{i=1}^{m} x^i - \\Sigma_{i=1}^{m}\\mu)$  \n",
    "$\\Sigma_{i=1}^{m}\\mu = \\Sigma_{i=1}^{m} x^i$  \n",
    "$\\hat{\\mu} = \\frac{1}{m} \\Sigma_{i=1}^{m} x^i $\n",
    "  \n",
    "  \n",
    "$\\frac{\\partial l}{\\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2} [\\frac{-m}{2}log(\\sigma^2)- \\frac{\\Sigma_{i=1}^{m}(x^i-\\mu)^2}{2\\sigma^2}] = 0$  \n",
    "Skipping a few algebraic steps, the estimate for $\\hat\\sigma^2 = \\frac{1}{m} \\Sigma_{i=1}^{m}(x^i-\\mu)^2$\n",
    "\n",
    "To determine if these are maximum values, we have to take the second derivatives and show that it is negative in order for the estimates to be maximums.  \n",
    "$\\frac{\\partial^2 l}{\\partial \\mu^2} = \\frac{1}{\\sigma^2} (\\Sigma_{i=1}^{m} x^i - \\Sigma_{i=1}^{m}\\mu) = \\frac{-m\\mu}{\\sigma^2} \\longrightarrow$  this is always negative so $\\hat\\mu$ is a maximum\n",
    "\n",
    "$\\frac{\\partial^2 l}{\\partial (\\sigma^2)^2} = \\frac{-m}{2\\sigma^2} \\longrightarrow$  this is also always negative so $\\hat\\sigma^2$ is a maximum.\n",
    "\n",
    "\n",
    "**2. Please compare the pros and cons of KDE as opposed to histograms, and\n",
    "give at least one advantage and disadvantage to each.**\n",
    "\n",
    "Histograms\n",
    "- Advantage: Easy to interpret and understand, especially when presenting to non-technical stakeholders\n",
    "- Disadvantage: Output depends on where you put the bins so the estimates can become noisy and histogram could look sparse and difficult to interpret\n",
    "\n",
    "KDE\n",
    "- Advantage: Smaller errors when comparing estimated density function vs true density function\n",
    "- Disadvantage: Computationally heavy as you need to evaluate m functions\n",
    "\n",
    "**3. For the EM algorithm for GMM, please show how to use Bayes rule to drive\n",
    "$\\tau^i_k$ in closed-form expression.**\n",
    "\n",
    "Bayes Rule  \n",
    "$P(z|k) = \\frac{P(x|z)P(z)}{P(x)}$\n",
    "\n",
    "Following Bayes Rule:  \n",
    "$\\tau ^i_k := p(z^i=k | x^i, \\theta^i) = \\frac{p(x^i|z^i=k)p(z^i=k)}{\\Sigma_{k'=1...K}p(z^i=k', x^i)}$  \n",
    "$p(x|z) = p(x^i|z^i=k) = N(x|\\mu_k,\\Sigma_k)$  \n",
    "$p(z) = p(z^i=k)=\\pi_k$  \n",
    "$p(x) = \\Sigma_{k'=1...K}p(z^i=k', x^i) = \\Sigma_{k'=1...K}\\pi_{k'}N(x|\\mu_k',\\Sigma_k')$\n",
    "\n",
    "Thus, $\\tau ^i_k  = \\frac{\\pi_k N(x|\\mu_k,\\Sigma_k)}{\\Sigma_{k'=1...K}\\pi_{k'}N(x|\\mu_k',\\Sigma_k')}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194aa82",
   "metadata": {},
   "source": [
    "## 2. Density Estimation: Psychological Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ef25a",
   "metadata": {},
   "source": [
    "### Part A\n",
    "**Form the 1-dimensional histogram and KDE to estimate the distributions of amygdala and acc, respectively. For this question, you can ignore the variable orientation. Decide on a suitable number of bins so you can see the shape of the distribution clearly. Set an appropriate kernel bandwidth h > 0.**\n",
    "\n",
    "For the histograms, I chose 10 bins.\n",
    "\n",
    "|   | Histogram   | KDE     |\n",
    "|:--:|:-----------:|:-------:|\n",
    "| ACC | z | <img src=\"Q2_Output/PartA_acc_kde.jpg\" width=\"400\"/>| \n",
    "| Amygdala | <img src=\"Q2_Output/PartA_amygdala_hist.jpg\" width=\"400\"/> |<img src=\"Q2_Output/PartA_amygdala_kde.jpg\" width=\"400\"/>| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73774af5",
   "metadata": {},
   "source": [
    "### Part B\n",
    "**Form 2-dimensional histogram for the pairs of variables (amygdala, acc). Decide on a suitable number of bins so you can see the shape of the distribution clearly.**\n",
    "\n",
    "<img src=\"Q2_Output/PartB_2DHist.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3542431",
   "metadata": {},
   "source": [
    "### Part C\n",
    "**Use kernel-density-estimation (KDE) to estimate the 2-dimensional density function of (amygdala, acc) (this means for this question, you can ignore the variable orientation). Set an appropriate kernel bandwidth h > 0. Please show the two-dimensional KDE (e.g., two-dimensional heat-map, two-dimensional contour plot, etc.)**\n",
    "<img src=\"Q2_Output/PartC_2DKDE.jpg\" width=\"500\"/>\n",
    "\n",
    "**Please explain what you have observed: is the distribution unimodal or bi-modal? Are there any outliers?**  \n",
    "The data appears to be unimodal and does have some outliers.  \n",
    "\n",
    "**Please explain based on the results, can you infer that the two variables (amygdala, acc) are likely to be independent or not?**  \n",
    "To check if the two distributions are independent, I must check if $p(acc, amygdala) = p(acc)* p(amygdala)$.\n",
    "\n",
    "Below is the plot of the absolute error between the joint distribution and the product of the marginal distributions. If amygdala and acc are independent, I would expect this difference to be 0:\n",
    "<img src=\"Q2_Output/PartC_Error.jpg\" width=\"400\"/>\n",
    "\n",
    "Since the difference between the two distributions is not 0, I can conclude that amygdala and acc are not independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764744a",
   "metadata": {},
   "source": [
    "### Part D\n",
    "**We will consider the variable orientation and consider conditional distributions. Please plot the estimated conditional distribution of amygdala conditioning on political orientation: p(amygdala|orientation = c), c = 2, . . . , 5, using KDE. Set an appropriate kernel bandwidth h > 0. Do the same for the volume of the acc: plot p(acc|orientation = c), c = 2, . . . , 5 using KDE.**\n",
    "\n",
    "| Orientation   | ACC   | Amygdala     |\n",
    "|:--:|:-----------:|:-------:|\n",
    "| 2 | <img src=\"Q2_Output/PartD_acc_c2.jpg\" width=\"200\"/> | <img src=\"Q2_Output/PartD_amy_c2.jpg\" width=\"200\"/>| \n",
    "| 3 | <img src=\"Q2_Output/PartD_acc_c3.jpg\" width=\"200\"/> | <img src=\"Q2_Output/PartD_amy_c3.jpg\" width=\"200\"/>| \n",
    "| 4 | <img src=\"Q2_Output/PartD_acc_c4.jpg\" width=\"200\"/> | <img src=\"Q2_Output/PartD_amy_c4.jpg\" width=\"200\"/>| \n",
    "| 5 | <img src=\"Q2_Output/PartD_acc_c5.jpg\" width=\"200\"/> | <img src=\"Q2_Output/PartD_amy_c5.jpg\" width=\"200\"/>| \n",
    "\n",
    "**Now please explain based on the results, can you infer that the conditional distribution of amygdala and acc, respectively, are different from c = 2,...,5? This is a type of scientific question one could infer from the data: Whether or not there is a difference between brain structure and political view.**  \n",
    "Based on these graphs, I would infer that there is indeed a relationship between the size of brain structures and political views. The distribution of amygdala size differs dramatically given different political orientations. There is a difference in ACC distribution, but not as drastic compare to that of amygdala.\n",
    "\n",
    "**Conditional Sample Mean**\n",
    "\n",
    "|   | c = 2   | c = 3     | c = 4  | c = 5     |\n",
    "|:--:|:-----------:|:-------:|:-----------:|:-------:|\n",
    "| Amygdala   | 0.02   | 0     | 0  | 0.01     |\n",
    "| ACC  | -0.01   | 0    | 0  | -0.01     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145c42d",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "**Again we will consider the variable orientation. We will estimate the conditional joint distribution of the volume of the amygdala and acc, conditioning on a function of political orientation: p(amygdala, acc|orientation = c), c = 2, . . . , 5. You will use two-dimensional KDE to achieve the goal; et an appropriate kernel band- width h > 0. Please show the two-dimensional KDE (e.g., two-dimensional heat-map, two-dimensional contour plot, etc.).**\n",
    "\n",
    "| Orientation   | Joint Distribution   |\n",
    "|:--:|:-----------:|\n",
    "| 2 | <img src=\"Q2_Output/PartE_joint_c2.jpg\" width=\"200\"/> |  \n",
    "| 3 | <img src=\"Q2_Output/PartE_joint_c3.jpg\" width=\"200\"/> | \n",
    "| 4 | <img src=\"Q2_Output/PartE_joint_c4.jpg\" width=\"200\"/> |\n",
    "| 5 | <img src=\"Q2_Output/PartE_joint_c5.jpg\" width=\"200\"/> | \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd75b4",
   "metadata": {},
   "source": [
    "**Please explain based on the results, can you infer that the conditional distribution of two variables (amygdala, acc) are different from c = 2, . . . , 5? This is a type of scientific question one could infer from the data: Whether or not there is a difference between brain structure and political view.**  \n",
    "Based on the shape of the joint distributions, I can infer that the conditional distribution of amygdala and ACC are indeed different based on political view. It looks like the distributions for c=4,5 are similar in shape - perhaps these political orientations are more similar to each other than to c=2 or c=3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d21890",
   "metadata": {},
   "source": [
    "## 3. Implementing EM for MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5fc34",
   "metadata": {},
   "source": [
    "### Part A\n",
    "**Write down detailed expression of the E-step and M-step in the EM algorithm.**  \n",
    "Used Slide 13 in Module 7 notes as a reference.  \n",
    "\n",
    "Expectation Step: \n",
    "\n",
    "$\\tau^i_k = p(z^i = 1 | D, \\mu, \\Sigma) \n",
    "= \\frac{\\pi_k N(x^i|\\mu_k, \\Sigma_k)}{\\Sigma_{k'=1}^K \\pi_k' N(x^i|\\mu_k', \\Sigma_k')} \n",
    "= \\frac{\\frac{\\pi_k}{\\sqrt{|\\Sigma_k|}} e^{\\frac{-1}{2}(x^i-\\mu_k)^T\\Sigma_{k}^{-1}(x^i-\\mu_k)}}{\\Sigma_{k'=1}^K \\frac{\\pi_{k'}}{\\sqrt{|\\Sigma_{k'}|}} e^{\\frac{-1}{2}(x^i-\\mu_{k'})^T\\Sigma_{k'}^{-1}(x^i-\\mu_{k'})}}$  \n",
    "  \n",
    "Maximization Step:  \n",
    "Use the definition of $\\tau^i_k$ outlined in E step.\n",
    "\n",
    "$\\pi_k = \\frac{1}{m} \\Sigma_i \\tau^i_k$  \n",
    "  \n",
    "$\\mu_k = \\frac{\\Sigma_i \\tau^i_kx^i}{\\Sigma_i \\tau^i_k}$\n",
    "  \n",
    "$\\Sigma_k = \\frac {\\Sigma_i \\tau^i_k (x^i-\\mu_k)(x^i-\\mu_k)} {\\Sigma_i \\tau^i_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66961cf",
   "metadata": {},
   "source": [
    "### Part B\n",
    "**Implement EM algorithm yourself. Plot the log-likelihood function vs the number of iterations to show your algorithm is converging.**  \n",
    "EM algorithm converged in 27 iterations. \n",
    "<img src=\"Q3_Output/PartB_Loglikelihood.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46692b95",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "**Report the fitted GMM model when EM terminates. For the mean of each component, map these back to the original space and reformat the vectors to make them into 28-by-28 matrices and show images. Ideally, you should be able to see these means correspond to “average” images. You can report the two 4-by-4 covariance matrices by visualizing their intensities (e.g., using a gray scaled image or heat map).**\n",
    "\n",
    "$\\pi_1 = 0.48677765$, $\\pi_2 = 0.51322235$\n",
    "\n",
    "| Mean Images   |    |\n",
    "|:--:|:-----------:|\n",
    "|<img src=\"Q3_Output/PartC_Image1.jpg\" width=\"300\"/> |  <img src=\"Q3_Output/PartC_Image2.jpg\" width=\"300\">| \n",
    "\n",
    "| Covariance Heat Maps    |   |\n",
    "|:--:|:-----------:|\n",
    "|<img src=\"Q3_Output/PartC_Cov1.jpg\" width=\"300\"/>|  <img src=\"Q3_Output/PartC_Cov2.jpg\" width=\"300\"/> | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a0c5c",
   "metadata": {},
   "source": [
    "### Part D\n",
    "**Use τki to infer the labels of the images, and compare with the true labels. Report the mis-classification rate for digits “2” and “6” respectively. Perform K-means clustering with K = 2 (you may call a package or use code from previous assignments). Find the mis-classification rate for digits “2” and “6” respectively, and compare with GMM. Which model achieves better performance overall?** \n",
    "\n",
    "GMM Misclassification Rate = 0.03769  \n",
    "K Means Misclassification Rate = 0.06231\n",
    "\n",
    "The EM algorithm achieves the best performance overall. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
