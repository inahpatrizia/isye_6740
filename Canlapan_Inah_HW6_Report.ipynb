{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4857f5c6",
   "metadata": {},
   "source": [
    "# HW6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34a274",
   "metadata": {},
   "source": [
    "### 1. Conceptual Questions\n",
    "\n",
    "**a. Explain how we can control the data-fit complexity for regression trees.**  \n",
    "The data-fit complexity for regression trees can be controlled by growing a large tree and stop when a minimum node size has been reached. Then prune the tree by minimizing the cost function:\n",
    "$C_\\alpha(S)= \\Sigma_{j=1}^{|J|}\\Sigma_{x_i\\in R_j} (y_i-\\hat c_j)^2 + \\alpha|J|$    \n",
    "\n",
    "**b. What's the main difference between boosting and bagging?**  \n",
    "In boosting, weak learners learn sequentially and focus on improving the results of the previous learner. The errors from the previous learner are more heavily weighted than other data in current learner's training set.  The learners are combined linearly.\n",
    "In bagging, no prior knowledge from other learners is required. Weak learners are run in parallel on bootstrap replicates of the training set. The learners are combined by taking their average.    \n",
    "\n",
    "**c. Explain how OOB errors are constructed and how to use them to decide upon an ideal number of trees in random forest. Is OOB error test or training error and why?**  \n",
    "Out of bag errors are constructed when bagging is used to train a random forest. Each tree is trained using a bootstrap sample. Some data is left out to create an out of bag sample. The OOB sample is then used as test data for the trees that do not contain the OOB sample in their training data. The majority vote or average prediction from the group of trees is used to determine the classification. OOB errors is the number of incorrectly predicted rows from the OOB sample. To decide upon an ideal number of trees, we observe when the OOB error stabilizes; we can stop training when OOB error is stable. Since the data used to test the model is different from the data used to train the model, OOB error would be considered a test error.  \n",
    "\n",
    "**d. Explain what the \"kernel trick\" is and why it is used.**  \n",
    "The kernel trick allows us to use the inner product of features rather than considering all the features explicitly. The feature space can grow very large, very quickly, can be computationally expensive and may require lots of data to fit all the polynomial terms. By using the kernel trick, we are able to simplify the feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501d497",
   "metadata": {},
   "source": [
    "### 2. AdaBoost\n",
    "**a. For each iteration t=1,2,3 compute $\\epsilon_t, \\alpha_t, Z_t, D_t$ by hand.**   \n",
    "I will show the calculations for the first iteration. Complete table of calculations can be found in *2_AdaBoost.xlsx*.  \n",
    "\n",
    "$D_1(i) = \\frac{1}{m} = \\frac{1}{8}, i=1,2,3,4,5,6,7,8$  \n",
    "$\\epsilon_1 = \\frac{1}{8}(1+1) = \\frac{1}{4} \\longrightarrow X_5, X_6$ are misclassified  \n",
    "$\\alpha_1 = \\frac{1}{2}ln(\\frac{1-\\epsilon_1}{\\epsilon_1}) = \\frac{1}{2}ln(\\frac{1-0.25}{0.25}) \\approx 0.5493 $  \n",
    "$Z_t = \\Sigma_{i=1}^m D_t(i)e^{-\\alpha_ty^ih_t(x^i)} = \\frac{1}{8}[6e^{-0.5493} + 2e^{0.5493}] \\approx 0.8660$   \n",
    "$D_2(i)=\\frac{D_t(i)}{Z_t}e^{-\\alpha_ty^ih_t(x^i)} \\longrightarrow D_2(1)=D_2(2)=D_2(3)=D_2(4)=D_2(7)=D_2(8)=0.0833, D_2(5)=D_2(6)=0.25$\n",
    "  \n",
    "  \n",
    "**Decision Stumps**\n",
    "\n",
    "|t=1|t=2|t=3|\n",
    "|:-:|:-:|:-:|\n",
    "|<img src=\"2_Adaboost_t1.png\" width=\"250\"/>|<img src=\"2_Adaboost_t2.png\" width=\"250\"/>|<img src=\"2_Adaboost_t3.png\" width=\"250\"/>|\n",
    "\n",
    "**Final Classification**  \n",
    "$H(x) = sign(\\alpha_1h_1(x)+\\alpha_2h_2(x)+\\alpha_3h_3(x)) = sign(0.25h_1(x)+0.8047h_2(x)+1.0986h_3(x))$  \n",
    "$H(1) = sign(0.25*1+0.8047*-1+1.0986*1) = 1$  \n",
    "$H(2) = sign(0.25*1+0.8047*-1+1.0986*1) = 1$  \n",
    "$H(3) = sign(0.25*-1+0.8047*-1+1.0986*-1) = -1$  \n",
    "$H(4) = sign(0.25*-1+0.8047*-1+1.0986*-1) = -1$\n",
    "$H(5) = sign(0.25*-1+0.8047*1+1.0986*1) = 1$  \n",
    "$H(6) = sign(0.25*-1+0.8047*1+1.0986*1) = 1$  \n",
    "$H(7) = sign(0.25*-1+0.8047*-1+1.0986*1) = -1$  \n",
    "$H(8) = sign(0.25*-1+0.8047*-1+1.0986*1) = -1$\n",
    "\n",
    "<img src=\"2_Adaboost_Results.png\"/>\n",
    "\n",
    "**b. What is the training error of this AdaBoost model? Give a short explanation for why AdaBoost outperforms a single decision stump.**   \n",
    "\n",
    "Based on the final classification, the training error for this AdaBoost model is 0 - all data points were classified correctly. The average training error for all iterations is 0.1722 and the final training error achieved in t=3 is 0.1.  \n",
    "\n",
    "AdaBoost outperforms a single decision stump because it allows for future weak learners to learn from the weighted dataset from the previous weak learner, thus creating a single strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c86a2c",
   "metadata": {},
   "source": [
    "### 3. Random Forest and One-Class SVM for Email Spam Classifier\n",
    "Your task for this question is to build a spam classifier using the UCR email spam dataset https: //archive.ics.uci.edu/ml/datasets/Spambase came from the postmaster and individuals who had filed spam. Please download the data from that website.   \n",
    "\n",
    "The collection of non-spam emails came from filed work and personal emails, and hence the word ’george’ and the area code ’650’ are indicators of non-spam. These are useful when constructing a personalized spam filter. You are free to choose any package for this homework. Note: there may be some missing values. You can just fill in zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba7c57",
   "metadata": {},
   "source": [
    "**a. Randomly shuffle the data and partition to use 80% for training and the remaining 20% for testing. Build a CART model with the training data and visualize the fitted classification.**  \n",
    "\n",
    "To create this tree, I configured the classifier to have min_samples_leaf = 100. Without this setting, the tree was very large and difficult to read and understand.   \n",
    "<img src=\"Q3a_Full_Tree.jpg\"/>  \n",
    "\n",
    "Below is a snapshot of the first 4 levels:\n",
    "<img src=\"Q3a_Partial_Tree.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b53d8",
   "metadata": {},
   "source": [
    "**b. Now also build a random forest model. Use your train/test split from part a). Compare and report the test error for your classification tree and random forest models on testing data. Plot the curve of test error (total misclassification error rate) versus the number of trees for the random forest, and plot the test error for the CART model (which should be a constant with respect to the number of trees).**  \n",
    "\n",
    "CART Test Error = 0.11943539630836053  \n",
    "Random Forest Test Error = 0.09880564603691644  \n",
    "\n",
    "Test Error vs Number of Trees\n",
    "<img src=\"Q3b_Error_Rate.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5d8a9",
   "metadata": {},
   "source": [
    "**c. Now we will use a one-class SVM approach for spam filtering. Use your train/test split from part a). Extract all non-spam emails from the training block (80% of data you have selected) to build the one-class kernel SVM using RBF kernel (you can turn the kernel bandwidth to achieve good performance). Then apply it on the 20% of data reserved for testing (thus this is a novelty detection situation), and report the total misclassification error rate on this testing data.**  \n",
    "\n",
    "Total Misclassification Error Rate = 0.3669923995656895"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52c740",
   "metadata": {},
   "source": [
    "### 4. Locally Weighted Linear Regression and Bias-Variance Tradeoff\n",
    "Consider a data set with n data points ($x_i,y_i$) following the following linear model:  \n",
    "$y_i=\\beta^{*T}x_u+\\epsilon_i$  \n",
    "where $\\epsilon_i$ ~ $N(0, \\sigma_i^2)$ are independent (but not identically distributed) Gaussian noise with zero mean and variance $\\sigma_i^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40bbd77",
   "metadata": {},
   "source": [
    "**a. Show that the ridge regression which introduces a squared l2 norm penalty on the\n",
    "parameter in the maximum likelihood estimate of β can be written as follows:\n",
    "$\\hat\\beta(\\lambda) = argmin_\\beta \\{X(\\beta-y)^TWX(\\beta-y)+\\lambda||\\beta||_2^2\\}$ for diagonal matrix W, matrix X and vector y.\n",
    "Please also explain how W, X and y are defined in terms of $\\sigma_i^2$, $x_i$ and $y_i$.**  \n",
    "\n",
    "*I'm following steps from Prof. Xie's office hour from March 29.*\n",
    "\n",
    "Start with the log likelihood function, given that $y_i$ ~ $N(B^{*T}x_i, \\sigma_i^2 )$:  \n",
    "$\\Sigma_{i=1}^n log(f(y_i|\\beta))$  \n",
    "= $\\Sigma_{i=1}^n log(\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{\\frac{-(y_i-\\beta^Tx_i)^2}{2\\sigma_i^2}} )$  \n",
    "$\\longrightarrow l(\\beta) = \\Sigma_{i=1}^n \\frac{-1}{2}log(2\\pi\\sigma_i^2) - \\Sigma_{i=1}^n \\frac{(y_i-\\beta^Tx_i)^2}{2\\sigma_i^2}$  \n",
    "\n",
    "To get the maximum likelihood estimate of $\\beta$, we will add the ridge regression penalty and take the derivative of $l(\\beta)$ and set to 0. We can drop the first term because it doesn't depend on $\\beta$.  \n",
    "\n",
    "$max_\\beta\\;l(\\beta) = - \\Sigma_{i=1}^n \\frac{(y_i-\\beta^Tx_i)^2}{2\\sigma_i^2} + \\lambda ||\\beta||_2^2$  \n",
    "$\\longrightarrow min_\\beta\\;l(\\beta) = \\Sigma_{i=1}^n \\frac{(y_i-\\beta^Tx_i)^2}{2\\sigma_i^2} + \\lambda ||\\beta||_2^2$  \n",
    "Let $y = \\begin{bmatrix} y_1 \\\\ ... \\\\ y_n \\end{bmatrix}, X = \\begin{bmatrix} x_1^T \\\\ ... \\\\ x_n^T \\end{bmatrix}$ and $X\\beta = \\begin{bmatrix} x_1^T\\beta \\\\ ... \\\\ x_n^T\\beta \\end{bmatrix}$ and W = covariance matrix = $\\begin{bmatrix} \\frac{1}{2\\sigma_1^2}...0 \\\\ ... \\\\ 0...\\frac{1}{2\\sigma_n^2} \\end{bmatrix}$\n",
    "\n",
    "\n",
    "Then $\\Sigma_{i=1}^n \\frac{(y_i-\\beta^Tx_i)^2}{2\\sigma_i^2} \\equiv (y-X\\beta)^TW(y-X\\beta)$  \n",
    "and the expression $min_\\beta\\;l(\\beta) = \\Sigma_{i=1}^n \\frac{(y_i-\\beta^Tx_i)^2}{2\\sigma_i^2} + \\lambda ||\\beta||_2^2 \\equiv min_\\beta\\;l(\\beta) = (y-X\\beta)^TW(y-X\\beta) + \\lambda ||\\beta||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f85df2",
   "metadata": {},
   "source": [
    "**b. Find the closed-form solution for β(λ) and its distribution conditioning on {$x_i$}.**  \n",
    "\n",
    "To find closed form, we must take the derivative of $l(\\beta) = (y-X\\beta)^TW(y-X\\beta) + \\lambda ||\\beta||_2^2$ and set to 0.  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\beta} (y-X\\beta)^TW(y-X\\beta) + \\lambda ||\\beta||_2^2 = -2WX^T(y-X\\beta) + 2\\lambda\\beta$  \n",
    "$0 = -2WX^Ty + 2WX^TX\\beta + 2\\lambda\\beta$  \n",
    "$2WX^Ty = \\beta(2WX^TX + 2\\lambda)$  \n",
    "$\\longrightarrow \\hat\\beta = \\frac{WX^Ty}{WX^TX + \\lambda}$  \n",
    "\n",
    "Finding the distribution:  \n",
    "$E[\\hat\\beta] = E[\\frac{WX^Ty}{WX^TX + \\lambda}] = \\frac{WX^T}{WX^TX + \\lambda}*E[y] = \\frac{WX^T}{WX^TX + \\lambda}* (X\\beta^*)$  \n",
    "$Var(\\hat\\beta) = Var(\\frac{WX^Ty}{WX^TX + \\lambda}) = \\frac{WX^T}{WX^TX + \\lambda}*Var(y) = \\frac{WX^T}{WX^TX + \\lambda}\\sigma^2$, where $\\sigma^2 = \\begin{bmatrix} \\sigma_1^2 \\\\ ... \\\\ \\sigma_n^2 \\end{bmatrix}$  \n",
    "\n",
    "Thus, $\\hat\\beta$ \\~ $N(\\frac{WX^TX\\beta^*}{WX^TX + \\lambda}, \\frac{WX^T\\sigma^2}{WX^TX + \\lambda})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff57a2",
   "metadata": {},
   "source": [
    "**c. Derive the bias as a function of λ and some fixed test point x.**  \n",
    "\n",
    "Bias = $E[\\hat y] - y = E[X\\hat\\beta] - X\\beta^*$  \n",
    "$= E[X\\frac{WX^TX\\beta^*}{WX^TX + \\lambda}] - X\\beta^*$  \n",
    "$= X\\frac{WX^TX\\beta^*}{WX^TX + \\lambda} - X\\beta^*$  \n",
    "$= \\frac{XWX^TX\\beta^*- X\\beta^*(WX^TX + \\lambda)}{WX^TX + \\lambda}$   \n",
    "$=  \\frac{XWX^TX\\beta^*- X\\beta^*WX^TX - X\\beta^*\\lambda}{WX^TX + \\lambda} $  \n",
    "$= \\frac{- X\\beta^*\\lambda}{WX^TX + \\lambda} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60a420",
   "metadata": {},
   "source": [
    "**d. Derive the variance term as a function of λ and some fixed text point x.**  \n",
    "\n",
    "Variance = $Var(\\hat y) = Var(x^T\\hat\\beta) = x^TVar(\\hat\\beta)x$  \n",
    "$= \\frac{x^TWX^T\\sigma^2x}{WX^TX + \\lambda}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed0469",
   "metadata": {},
   "source": [
    "**e. Now assuming the data are one-dimensional, we provide a toy training dataset of two samples: $x_1$ = 1.5 and $x_2$ = 1, and the test sample x = 1.7. The true parameter $\\beta_0^*$ = 1, $\\beta_1^*$ = 1, the noise variance is given by $\\sigma_1^2$ = 2, $\\sigma_2^2$ = 1. Plot the MSE (Bias square plus variance) as a function of the regularization parameter λ.**  \n",
    "\n",
    "$X = \\begin{bmatrix} 1.5 \\\\ 1 \\end{bmatrix}, \\;  \\beta^* = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\; \n",
    "W = \\begin{bmatrix} \\frac{1}{2\\sigma_1^2} & 0 \\\\ 0 & \\frac{1}{2\\sigma_2^2}  \\end{bmatrix}\n",
    "= \\begin{bmatrix} \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{2}  \\end{bmatrix}, \\;\n",
    "\\sigma^2 = \\begin{bmatrix} \\sigma_1^2 \\\\ \\sigma_2^2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "$MSE = Bias^2+ Variance = (\\frac{- X\\beta^*\\lambda}{WX^TX + \\lambda})^2 + \\frac{x^TWX^T\\sigma^2x}{WX^TX + \\lambda}$  \n",
    "\n",
    "*I know that my derivations are not correct because I cannot get a scalar value from my bias and variance calculations. In particular, I cannot get a scalar value for the denominator.  \n",
    "For the sake of this problem and to generate some graphs, I will plot the spectral norm of MSE, bias and variance matrices I get as a result of my calculation.*   \n",
    "\n",
    "<img src=\"Q4e_Output.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3eab6",
   "metadata": {},
   "source": [
    "**f. Now change the test sample to be a x = 2.5, and keep everything else the same as in the previous question. Plot the MSE (Bias square plus variance) as a function of the regularization parameter λ, and comment on the difference from the previous result.**  \n",
    "\n",
    "As stated in part e, my derivations are incorrect so I cannot properly comment on the difference between x=1.7 and x=2.5.\n",
    "\n",
    "<img src=\"Q4f_Output.jpg\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
